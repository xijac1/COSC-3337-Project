{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBLP Anomaly Detection\n\n**Team Member:** Julio Amaya\n\nDetecting outliers, off-topic venues, and atypical collaborations in the DBLP dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport networkx as nx\nimport warnings\n\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (12, 6)\nsns.set_style('whitegrid')\n\nproject_root = Path('.').resolve()\nsys.path.insert(0, str(project_root))\n\nprint('\u2713 Imports loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = project_root / 'data' / 'parquet'\nOUTPUT_DIR = project_root / 'data' / 'derived'\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\npapers = pd.read_parquet(DATA_DIR / 'papers')\ncitations_df = pd.read_parquet(DATA_DIR / 'citations')\ncoauthorships_df = pd.read_parquet(DATA_DIR / 'coauthorships')\nauthorships = pd.read_parquet(DATA_DIR / 'authorships')\n\nprint(f'Papers:         {papers.shape[0]:>10,} rows')\nprint(f'Citations:      {citations_df.shape[0]:>10,} rows')\nprint(f'Coauthorships:  {coauthorships_df.shape[0]:>10,} rows')\nprint(f'Authorships:    {authorships.shape[0]:>10,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Citation Outliers: Extreme Citation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = citations_df.groupby('dst_id').size().reset_index(name='in_degree')\ncitation_stats = papers[['id', 'title', 'year', 'venue', 'n_citation']].copy()\ncitation_stats = citation_stats.merge(in_degree, left_on='id', right_on='dst_id', how='left').fillna(0)\ncitation_stats['in_degree'] = citation_stats['in_degree'].astype(int)\ncitation_stats['citation_ratio'] = citation_stats['n_citation'] / (citation_stats['in_degree'] + 1)\n\nscaler = StandardScaler()\ncitation_stats['z_citation'] = np.abs(scaler.fit_transform(citation_stats[['n_citation']]))\ncitation_stats['z_indegree'] = np.abs(scaler.fit_transform(citation_stats[['in_degree']]))\n\ncitation_outliers = citation_stats[(citation_stats['z_citation'] > 3) | (citation_stats['z_indegree'] > 3)].copy()\n\nprint(f'\u2713 Citation Outliers: {len(citation_outliers):,}')\nprint('\\nTop 10 Citation Outliers:')\nif len(citation_outliers) > 0:\n    display(citation_outliers.nlargest(10, 'n_citation')[['title', 'year', 'venue', 'n_citation', 'in_degree']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Venue Anomalies: Off-Topic Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_clean = papers.dropna(subset=['title', 'venue']).copy()\nprint(f'Papers with title and venue: {len(papers_clean):,}')\n\nvectorizer = TfidfVectorizer(max_features=100, stop_words='english')\ntfidf_matrix = vectorizer.fit_transform(papers_clean['title'])\n\nvenue_anomalies = []\nfor venue in papers_clean['venue'].value_counts().head(20).index:\n    venue_papers = papers_clean[papers_clean['venue'] == venue]\n    if len(venue_papers) < 10:\n        continue\n    venue_tfidf = tfidf_matrix[venue_papers.index]\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    scores = iso_forest.fit_predict(venue_tfidf.toarray())\n    for idx, s in zip(venue_papers.index, scores):\n        if s == -1:\n            venue_anomalies.append({'venue': venue, 'paper_id': papers_clean.loc[idx, 'id']})\n\nvenue_anomalies_df = pd.DataFrame(venue_anomalies)\nprint(f'\u2713 Venue Anomalies: {len(venue_anomalies_df):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Co-authorship Cliques: Atypical Collaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building co-authorship graph...')\nG = nx.Graph()\nedges = list(zip(coauthorships_df['author1_norm'], coauthorships_df['author2_norm']))\nG.add_edges_from(edges)\n\nprint(f'\u2713 Graph: {G.number_of_nodes():,} authors, {G.number_of_edges():,} collaborations')\n\nprint('Computing cliques...')\nlargest_cc = max(nx.connected_components(G), key=len)\nG_sub = G.subgraph(largest_cc).copy()\n\ncliques = sorted(nx.find_cliques(G_sub), key=len, reverse=True)\nprint(f'\u2713 Total cliques: {len(cliques):,}')\nif cliques:\n    print(f'  Max clique size: {len(cliques[0])}')\n\nanomaly_cliques = []\nfor clique in cliques[:1000]:\n    if 3 <= len(clique) <= 8:\n        sg = G_sub.subgraph(clique)\n        density = nx.density(sg)\n        if density > 0.7:\n            anomaly_cliques.append({'size': len(clique), 'density': round(density, 3)})\n\nif anomaly_cliques:\n    df_cliques = pd.DataFrame(anomaly_cliques)\n    print(f'\u2713 Dense cliques (density > 0.7): {len(df_cliques)}')\nelse:\n    print('\u2713 No highly anomalous cliques detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(citation_stats['n_citation'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\naxes[0].set_xlabel('Citation Count')\naxes[0].set_ylabel('Frequency (log scale)')\naxes[0].set_title('Citation Count Distribution')\naxes[0].set_yscale('log')\naxes[0].grid(alpha=0.3)\n\naxes[1].scatter(citation_stats['in_degree'], citation_stats['n_citation'], alpha=0.5, s=10, label='All papers')\nif len(citation_outliers) > 0:\n    axes[1].scatter(citation_outliers['in_degree'], citation_outliers['n_citation'], \n                    color='red', s=100, alpha=0.7, label='Outliers', edgecolor='darkred')\naxes[1].set_xlabel('In-Degree (Citation Edges)')\naxes[1].set_ylabel('n_citation (Self-Reported)')\naxes[1].set_title('Citation Patterns: In-Degree vs n_citation')\naxes[1].set_xscale('log')\naxes[1].set_yscale('log')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\nsummary_df = pd.DataFrame({\n    'Type': ['Citation Outliers', 'Venue Anomalies', 'Co-authorship Cliques'],\n    'Count': [len(citation_outliers), len(venue_anomalies_df), len(anomaly_cliques) if anomaly_cliques else 0]\n})\n\nprint('\\n' + '='*60)\nprint('ANOMALY DETECTION SUMMARY')\nprint('='*60)\nprint(summary_df.to_string(index=False))\nprint('='*60)\n\n# Export CSVs\ncitation_outliers[['id', 'title', 'year', 'venue', 'n_citation', 'in_degree']].to_csv(\n    OUTPUT_DIR / 'anomalies_citation_outliers.csv', index=False\n)\nprint(f'\\n\u2713 Exported: anomalies_citation_outliers.csv ({len(citation_outliers)} rows)')\n\nif len(venue_anomalies_df) > 0:\n    venue_anomalies_df.to_csv(OUTPUT_DIR / 'anomalies_venue_offTopic.csv', index=False)\n    print(f'\u2713 Exported: anomalies_venue_offTopic.csv ({len(venue_anomalies_df)} rows)')\n\nsummary_df.to_csv(OUTPUT_DIR / 'anomaly_detection_summary.csv', index=False)\nprint(f'\u2713 Exported: anomaly_detection_summary.csv')\nprint(f'\\n\u2713 All results saved to data/derived/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}