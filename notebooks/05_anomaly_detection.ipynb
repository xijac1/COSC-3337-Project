{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBLP Anomaly Detection\n\n**Team:** Julio Amaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport networkx as nx\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pathlib import Path\n\nproject_root = Path('.').resolve()\nsys.path.insert(0, str(project_root))\nplt.rcParams['figure.figsize'] = (12, 6)\nsns.set_style('whitegrid')\nprint('\u2713 Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = project_root / 'data' / 'parquet'\nOUTPUT_DIR = project_root / 'data' / 'derived'\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\npapers = pd.read_parquet(DATA_DIR / 'papers')\ncitations_df = pd.read_parquet(DATA_DIR / 'citations')\ncoauthorships_df = pd.read_parquet(DATA_DIR / 'coauthorships')\n\nprint(f'Papers: {papers.shape[0]:,}')\nprint(f'Citations: {citations_df.shape[0]:,}')\nprint(f'Coauthorships: {coauthorships_df.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Citation Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = citations_df.groupby('dst_id').size().reset_index(name='in_degree')\ncitation_stats = papers[['id', 'title', 'year', 'venue', 'n_citation']].copy()\ncitation_stats = citation_stats.merge(in_degree, left_on='id', right_on='dst_id', how='left').fillna(0)\ncitation_stats['in_degree'] = citation_stats['in_degree'].astype(int)\n\nscaler = StandardScaler()\ncitation_stats['z_citation'] = np.abs(scaler.fit_transform(citation_stats[['n_citation']]))\ncitation_stats['z_indegree'] = np.abs(scaler.fit_transform(citation_stats[['in_degree']]))\n\ncitation_outliers = citation_stats[(citation_stats['z_citation'] > 3) | (citation_stats['z_indegree'] > 3)].copy()\nprint(f'Citation Outliers: {len(citation_outliers):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Venue Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_clean = papers.dropna(subset=['title', 'venue']).copy()\nvectorizer = TfidfVectorizer(max_features=100, stop_words='english')\ntfidf_matrix = vectorizer.fit_transform(papers_clean['title'])\n\nvenue_anomalies = []\nfor venue in papers_clean['venue'].value_counts().head(20).index:\n    venue_papers = papers_clean[papers_clean['venue'] == venue]\n    if len(venue_papers) >= 10:\n        venue_tfidf = tfidf_matrix[venue_papers.index]\n        iso_forest = IsolationForest(contamination=0.05, random_state=42)\n        scores = iso_forest.fit_predict(venue_tfidf.toarray())\n        venue_anomalies.extend([{'venue': venue} for idx, s in zip(venue_papers.index, scores) if s == -1])\n\nvenue_anomalies_df = pd.DataFrame(venue_anomalies)\nprint(f'Venue Anomalies: {len(venue_anomalies_df):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(citation_stats['n_citation'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\naxes[0].set_xlabel('Citation Count')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Citation Distribution')\naxes[0].set_yscale('log')\naxes[0].grid(alpha=0.3)\n\naxes[1].scatter(citation_stats['in_degree'], citation_stats['n_citation'], alpha=0.5, s=10)\nif len(citation_outliers) > 0:\n    axes[1].scatter(citation_outliers['in_degree'], citation_outliers['n_citation'], \n                    color='red', s=100, alpha=0.7, label='Outliers')\naxes[1].set_xlabel('In-Degree')\naxes[1].set_ylabel('n_citation')\naxes[1].set_title('Citation Patterns')\naxes[1].set_xscale('log')\naxes[1].set_yscale('log')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'anomaly_analysis_plot.png', dpi=150, bbox_inches='tight')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n    'Type': ['Citation Outliers', 'Venue Anomalies'],\n    'Count': [len(citation_outliers), len(venue_anomalies_df)]\n})\n\nprint('\\n' + '='*50)\nprint('ANOMALY DETECTION SUMMARY')\nprint('='*50)\nprint(summary.to_string())\nprint('='*50)\n\ncitation_outliers[['id', 'title', 'year', 'venue', 'n_citation', 'in_degree']].to_csv(\n    OUTPUT_DIR / 'anomalies_citation_outliers.csv', index=False)\nvenue_anomalies_df.to_csv(OUTPUT_DIR / 'anomalies_venue_offTopic.csv', index=False)\nsummary.to_csv(OUTPUT_DIR / 'anomaly_detection_summary.csv', index=False)\n\nprint('\\n\u2713 Results exported to data/derived/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}