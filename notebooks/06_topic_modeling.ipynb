{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c348145",
   "metadata": {},
   "source": [
    "# DBLP Topic Modeling & Trends Analysis\n",
    "\n",
    "**Team Member:** Julio Amaya  \n",
    "**Task:** NLP & Topic Modeling  \n",
    "**Date:** December 4, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs topic modeling on DBLP research abstracts to discover research themes and track their evolution:\n",
    "\n",
    "1. Load and preprocess paper abstracts\n",
    "2. Generate TF-IDF features\n",
    "3. Train topic models (LDA and NMF)\n",
    "4. Extract and label topics\n",
    "5. Analyze topic evolution over time\n",
    "6. Visualize topic distributions by venue and year\n",
    "7. Track topic trends and novelty\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cc377",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e205f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# NLP Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# Utilities\n",
    "from tqdm.notebook import tqdm\n",
    "import swifter  # For faster processing\n",
    "\n",
    "# Plotting Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c171365",
   "metadata": {},
   "source": [
    "**âš ï¸ IMPORTANT:** If you've updated `models.py`, restart the kernel (Kernel â†’ Restart) to reload the module with the latest changes including tqdm progress bars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd351b",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Set to True to test quickly on 10% of data. \n",
    "# Set to False to run the full analysis (may take 1-2 hours on a laptop).\n",
    "USE_SAMPLE_DATA = False\n",
    "SAMPLE_FRACTION = 0.10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Modeling Config\n",
    "N_TOPICS = 12\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "# --- PATH SETUP ---\n",
    "# Defines paths relative to where this notebook is located\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "FIGURES_DIR = OUTPUT_DIR / \"figures\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "\n",
    "# Create output directories automatically\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“‚ Looking for data in: {DATA_DIR}\")\n",
    "print(f\"ğŸ’¾ Saving results to:  {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22219a7d",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Standardizes text: lowercase, removes URLs/emails, \n",
    "    removes special chars, and normalizes accents.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase and standard clean\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove accents\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(ch for ch in text if not unicodedata.combining(ch))\n",
    "    return text.strip()\n",
    "\n",
    "def train_topic_model(tfidf_matrix, n_topics, algorithm='lda'):\n",
    "    \"\"\"Trains LDA or NMF model.\"\"\"\n",
    "    print(f\"âš™ï¸ Training {algorithm.upper()} model with {n_topics} topics...\")\n",
    "    \n",
    "    if algorithm == 'lda':\n",
    "        model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=RANDOM_SEED,\n",
    "            learning_method='online',\n",
    "            batch_size=256,\n",
    "            n_jobs=-1, # Use all CPU cores\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        model = NMF(\n",
    "            n_components=n_topics,\n",
    "            random_state=RANDOM_SEED,\n",
    "            init='nndsvda',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "    model.fit(tfidf_matrix)\n",
    "    return model\n",
    "\n",
    "def get_top_keywords(model, vectorizer, n_words=10):\n",
    "    \"\"\"Returns a dictionary of top words per topic.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[-n_words:][::-1]\n",
    "        topics_dict[topic_idx] = [feature_names[i] for i in top_indices]\n",
    "    return topics_dict\n",
    "\n",
    "print(\"âœ… Functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efb856",
   "metadata": {},
   "source": [
    "## 4. Generate TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "if not (DATA_DIR / 'papers').exists() and not (DATA_DIR / 'papers.parquet').exists():\n",
    "    raise FileNotFoundError(f\"Could not find 'papers.parquet' in {DATA_DIR}\")\n",
    "\n",
    "print(\"â³ Loading dataset...\")\n",
    "\n",
    "try:\n",
    "    # Try loading as a folder (parquet partition)\n",
    "    papers = pd.read_parquet(DATA_DIR / 'papers')\n",
    "except:\n",
    "    # Try loading as a single file\n",
    "    papers = pd.read_parquet(DATA_DIR / 'papers.parquet')\n",
    "\n",
    "print(f\"âœ… Total papers loaded: {len(papers):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e99ac",
   "metadata": {},
   "source": [
    "## 5. Train Topic Models\n",
    "\n",
    "We'll train both LDA and NMF models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter for valid abstracts\n",
    "df = papers[papers['abstract'].notna()].copy()\n",
    "df['raw_text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "\n",
    "# 2. Sampling (Optional)\n",
    "if USE_SAMPLE_DATA:\n",
    "    print(f\"âš ï¸ SAMPLING MODE: Using {SAMPLE_FRACTION*100}% of data.\")\n",
    "    df = df.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_SEED).copy()\n",
    "else:\n",
    "    print(\"ğŸš€ FULL MODE: Using all valid data.\")\n",
    "\n",
    "print(f\"Papers to process: {len(df):,}\")\n",
    "\n",
    "# 3. Text Cleaning\n",
    "print(\"ğŸ§¹ Cleaning text (this may take a while)...\")\n",
    "# Swifter speeds up pandas apply using your CPU cores\n",
    "df['text_clean'] = df['raw_text'].swifter.apply(clean_text)\n",
    "\n",
    "# 4. Remove artifacts (too short)\n",
    "df = df[df['text_clean'].str.len() > 30].copy()\n",
    "print(f\"Final count after cleaning: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3837db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”  Generating TF-IDF Features...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text_clean'])\n",
    "\n",
    "print(f\"TF-IDF Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0c935",
   "metadata": {},
   "source": [
    "## 6. Assign Topics to Documents\n",
    "\n",
    "Assign each paper to its dominant topic for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# NOTE: LDA is slower but often better for coherence. NMF is faster.\n",
    "lda_model = train_topic_model(tfidf_matrix, N_TOPICS, algorithm='lda')\n",
    "\n",
    "# Extract Keywords\n",
    "keywords = get_top_keywords(lda_model, vectorizer, n_words=10)\n",
    "\n",
    "print(\"\\n--- Discovered Topics ---\")\n",
    "for i, words in keywords.items():\n",
    "    print(f\"Topic {i}: {', '.join(words[:7])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21130c59",
   "metadata": {},
   "source": [
    "## 7. Visualize Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assigning topics to documents...\")\n",
    "\n",
    "# Get dominant topic for each paper\n",
    "# We process in chunks to be memory safe\n",
    "chunk_size = 2000\n",
    "topic_ids = []\n",
    "\n",
    "for i in tqdm(range(0, tfidf_matrix.shape[0], chunk_size)):\n",
    "    chunk = tfidf_matrix[i:i+chunk_size]\n",
    "    doc_topic_dist = lda_model.transform(chunk)\n",
    "    topic_ids.append(doc_topic_dist.argmax(axis=1))\n",
    "\n",
    "df['topic_id'] = np.hstack(topic_ids)\n",
    "\n",
    "# Map IDs to Labels\n",
    "topic_labels = {i: ', '.join(words[:3]) for i, words in keywords.items()}\n",
    "df['topic_label'] = df['topic_id'].map(topic_labels)\n",
    "\n",
    "# Save processed data\n",
    "output_file = OUTPUT_DIR / 'paper_topics.csv'\n",
    "df[['id', 'year', 'topic_id', 'topic_label']].to_csv(output_file, index=False)\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(lda_model, MODELS_DIR / 'lda_model.pkl')\n",
    "joblib.dump(vectorizer, MODELS_DIR / 'tfidf_vectorizer.pkl')\n",
    "\n",
    "print(f\"âœ… Data saved to: {output_file}\")\n",
    "print(f\"âœ… Models saved to: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39b1c5",
   "metadata": {},
   "source": [
    "## 8. Topic Evolution Over Time\n",
    "\n",
    "Analyze how topics have evolved and gained/lost popularity over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Topic Counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_counts = df['topic_id'].value_counts().sort_index()\n",
    "\n",
    "ax = sns.barplot(x=topic_counts.index, y=topic_counts.values, palette=\"viridis\")\n",
    "ax.set_title(f\"Paper Distribution per Topic (n={len(df):,})\")\n",
    "ax.set_xlabel(\"Topic ID\")\n",
    "ax.set_ylabel(\"Number of Papers\")\n",
    "\n",
    "# Save and Show\n",
    "plt.savefig(FIGURES_DIR / 'topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21240ff1",
   "metadata": {},
   "source": [
    "## 9. Summary and Export\n",
    "\n",
    "Save models and topic assignments for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f984d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'year' in df.columns:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filter valid years (Adjust range as needed)\n",
    "    temp_df = df[(df['year'] >= 2000) & (df['year'] <= 2017)]\n",
    "    \n",
    "    # Count per year/topic\n",
    "    trends = temp_df.groupby(['year', 'topic_id']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Get top 5 biggest topics\n",
    "    top_topics = df['topic_id'].value_counts().head(5).index\n",
    "    \n",
    "    # Plot\n",
    "    trends[top_topics].plot(marker='o', linewidth=2, ax=plt.gca())\n",
    "    \n",
    "    plt.title(\"Evolution of Top 5 Research Topics (2000-2017)\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.legend([topic_labels[i] for i in top_topics], title=\"Top Topics\", bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    plt.savefig(FIGURES_DIR / 'topic_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Year column not found, skipping trend plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
