{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c348145",
   "metadata": {},
   "source": [
    "# DBLP Topic Modeling & Trends Analysis\n",
    "\n",
    "**Team Member:** Julio Amaya  \n",
    "**Task:** NLP & Topic Modeling  \n",
    "**Date:** December 4, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs topic modeling on DBLP research abstracts to discover research themes and track their evolution:\n",
    "\n",
    "1. Load and preprocess paper abstracts\n",
    "2. Generate TF-IDF features\n",
    "3. Train topic models (LDA and NMF)\n",
    "4. Extract and label topics\n",
    "5. Analyze topic evolution over time\n",
    "6. Visualize topic distributions by venue and year\n",
    "7. Track topic trends and novelty\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cc377",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e205f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# NLP Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# Utilities\n",
    "from tqdm.notebook import tqdm\n",
    "import swifter  # For faster processing\n",
    "\n",
    "# Plotting Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c171365",
   "metadata": {},
   "source": [
    "**âš ï¸ IMPORTANT:** If you've updated `models.py`, restart the kernel (Kernel â†’ Restart) to reload the module with the latest changes including tqdm progress bars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd351b",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Define paths, parameters, and create output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Set to True to test quickly on 10% of data. \n",
    "# Set to False to run the full analysis (may take 1-2 hours on a laptop).\n",
    "USE_SAMPLE_DATA = False\n",
    "SAMPLE_FRACTION = 0.10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Modeling Config\n",
    "N_TOPICS = 12\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "# --- PATH SETUP ---\n",
    "# Defines paths relative to where this notebook is located\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "FIGURES_DIR = OUTPUT_DIR / \"figures\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "\n",
    "# Create output directories automatically\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“‚ Looking for data in: {DATA_DIR}\")\n",
    "print(f\"ğŸ’¾ Saving results to:  {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22219a7d",
   "metadata": {},
   "source": [
    "## 3. Define Utility Functions\n",
    "\n",
    "Create text preprocessing and topic modeling helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Standardizes text: lowercase, removes URLs/emails, \n",
    "    removes special chars, and normalizes accents.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase and standard clean\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove accents\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(ch for ch in text if not unicodedata.combining(ch))\n",
    "    return text.strip()\n",
    "\n",
    "def train_topic_model(tfidf_matrix, n_topics, algorithm='lda'):\n",
    "    \"\"\"Trains LDA or NMF model.\"\"\"\n",
    "    print(f\"âš™ï¸ Training {algorithm.upper()} model with {n_topics} topics...\")\n",
    "    \n",
    "    if algorithm == 'lda':\n",
    "        model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=RANDOM_SEED,\n",
    "            learning_method='online',\n",
    "            batch_size=256,\n",
    "            n_jobs=-1, # Use all CPU cores\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        model = NMF(\n",
    "            n_components=n_topics,\n",
    "            random_state=RANDOM_SEED,\n",
    "            init='nndsvda',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "    model.fit(tfidf_matrix)\n",
    "    return model\n",
    "\n",
    "def get_top_keywords(model, vectorizer, n_words=10):\n",
    "    \"\"\"Returns a dictionary of top words per topic.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[-n_words:][::-1]\n",
    "        topics_dict[topic_idx] = [feature_names[i] for i in top_indices]\n",
    "    return topics_dict\n",
    "\n",
    "print(\"âœ… Functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efb856",
   "metadata": {},
   "source": [
    "## 4. Load Papers Dataset\n",
    "\n",
    "Load the papers parquet file with abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  LOAD PAPERS â€“ Works with partitioned Parquet (your real setup)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Loading papers dataset from partitioned Parquet files...\")\n",
    "\n",
    "# This is the correct way for your project structure\n",
    "PAPERS_PATH = DATA_DIR / \"parquet\" / \"papers\"\n",
    "\n",
    "if not PAPERS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Could not find papers folder at {PAPERS_PATH}\")\n",
    "\n",
    "# This single line magically reads ALL part-*.parquet files at once\n",
    "papers = pd.read_parquet(PAPERS_PATH)\n",
    "\n",
    "print(f\"Total papers loaded: {len(papers):,}\")\n",
    "print(f\"Year range: {papers['year'].min():.0f} â€“ {papers['year'].max():.0f}\")\n",
    "print(f\"Papers with abstract: {papers['abstract'].notna().sum():,} ({papers['abstract'].notna().mean():.1%})\")\n",
    "\n",
    "# show small sample so you know it worked\n",
    "print(\"\\nSample:\")\n",
    "display(papers.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e99ac",
   "metadata": {},
   "source": [
    "## 5. Text Preprocessing\n",
    "\n",
    "Filter, sample (if enabled), and clean text data for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter for valid abstracts\n",
    "df = papers[papers['abstract'].notna()].copy()\n",
    "df['raw_text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "\n",
    "# 2. Sampling (Optional)\n",
    "if USE_SAMPLE_DATA:\n",
    "    print(f\"âš ï¸ SAMPLING MODE: Using {SAMPLE_FRACTION*100}% of data.\")\n",
    "    df = df.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_SEED).copy()\n",
    "else:\n",
    "    print(\"ğŸš€ FULL MODE: Using all valid data.\")\n",
    "\n",
    "print(f\"Papers to process: {len(df):,}\")\n",
    "\n",
    "# 3. Text Cleaning\n",
    "print(\"ğŸ§¹ Cleaning text (this may take a while)...\")\n",
    "# Swifter speeds up pandas apply using your CPU cores\n",
    "df['text_clean'] = df['raw_text'].swifter.apply(clean_text)\n",
    "\n",
    "# 4. Remove artifacts (too short)\n",
    "df = df[df['text_clean'].str.len() > 30].copy()\n",
    "print(f\"Final count after cleaning: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3837db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”  Generating TF-IDF Features...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text_clean'])\n",
    "\n",
    "print(f\"TF-IDF Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0c935",
   "metadata": {},
   "source": [
    "## 6. Train Topic Model (LDA)\n",
    "\n",
    "Train LDA model and extract top keywords for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# NOTE: LDA is slower but often better for coherence. NMF is faster.\n",
    "lda_model = train_topic_model(tfidf_matrix, N_TOPICS, algorithm='lda')\n",
    "\n",
    "# Extract Keywords\n",
    "keywords = get_top_keywords(lda_model, vectorizer, n_words=10)\n",
    "\n",
    "print(\"\\n--- Discovered Topics ---\")\n",
    "for i, words in keywords.items():\n",
    "    print(f\"Topic {i}: {', '.join(words[:7])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21130c59",
   "metadata": {},
   "source": [
    "## 7. Assign Topics to Documents\n",
    "\n",
    "Assign each paper to its dominant topic and save results with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assigning topics to documents...\")\n",
    "\n",
    "# Get dominant topic for each paper\n",
    "# We process in chunks to be memory safe\n",
    "chunk_size = 2000\n",
    "topic_ids = []\n",
    "\n",
    "for i in tqdm(range(0, tfidf_matrix.shape[0], chunk_size)):\n",
    "    chunk = tfidf_matrix[i:i+chunk_size]\n",
    "    doc_topic_dist = lda_model.transform(chunk)\n",
    "    topic_ids.append(doc_topic_dist.argmax(axis=1))\n",
    "\n",
    "df['topic_id'] = np.hstack(topic_ids)\n",
    "\n",
    "# Map IDs to Labels\n",
    "topic_labels = {i: ', '.join(words[:3]) for i, words in keywords.items()}\n",
    "df['topic_label'] = df['topic_id'].map(topic_labels)\n",
    "\n",
    "# Save processed data\n",
    "output_file = OUTPUT_DIR / 'paper_with_topics.csv'\n",
    "df[['id', 'year', 'topic_id', 'topic_label']].to_csv(output_file, index=False)\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(lda_model, MODELS_DIR / 'lda_model.pkl')\n",
    "joblib.dump(vectorizer, MODELS_DIR / 'tfidf_vectorizer.pkl')\n",
    "\n",
    "print(f\"âœ… Data saved to: {output_file}\")\n",
    "print(f\"âœ… Models saved to: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39b1c5",
   "metadata": {},
   "source": [
    "## 8. Visualize Topic Distribution\n",
    "\n",
    "Plot the number of papers assigned to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Topic Counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_counts = df['topic_id'].value_counts().sort_index()\n",
    "\n",
    "ax = sns.barplot(x=topic_counts.index, y=topic_counts.values, palette=\"viridis\")\n",
    "ax.set_title(f\"Paper Distribution per Topic (n={len(df):,})\")\n",
    "ax.set_xlabel(\"Topic ID\")\n",
    "ax.set_ylabel(\"Number of Papers\")\n",
    "\n",
    "# Save and Show\n",
    "plt.savefig(FIGURES_DIR / 'topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21240ff1",
   "metadata": {},
   "source": [
    "## 9. Topic Evolution Over Time\n",
    "\n",
    "Analyze how top research topics have evolved from 2000-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f984d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'year' in df.columns:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filter valid years (Adjust range as needed)\n",
    "    temp_df = df[(df['year'] >= 2000) & (df['year'] <= 2017)]\n",
    "    \n",
    "    # Count per year/topic\n",
    "    trends = temp_df.groupby(['year', 'topic_id']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Get top 5 biggest topics\n",
    "    top_topics = df['topic_id'].value_counts().head(5).index\n",
    "    \n",
    "    # Plot\n",
    "    trends[top_topics].plot(marker='o', linewidth=2, ax=plt.gca())\n",
    "    \n",
    "    plt.title(\"Evolution of Top 5 Research Topics (2000-2017)\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.legend([topic_labels[i] for i in top_topics], title=\"Top Topics\", bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    plt.savefig(FIGURES_DIR / 'topic_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Year column not found, skipping trend plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a92f8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully completed NLP-based topic modeling:\n",
    "\n",
    "1. **Setup**: Imported libraries and configured paths with optional sampling mode\n",
    "2. **Configuration**: Set topic count (12), max features (10,000), and output directories\n",
    "3. **Function Definitions**: Created text cleaning, model training, and keyword extraction utilities\n",
    "4. **Data Loading**: Loaded papers with abstracts from parquet files\n",
    "5. **Text Preprocessing**: Cleaned and filtered text (title + abstract), with swifter for parallel processing\n",
    "6. **TF-IDF Generation**: Created TF-IDF matrix with bigrams, min_df=5, max_df=0.9\n",
    "7. **LDA Training**: Trained Latent Dirichlet Allocation model with 12 topics using online learning\n",
    "8. **Topic Assignment**: Assigned dominant topic to each paper and saved to CSV\n",
    "9. **Visualizations**: Generated topic distribution bar chart and topic evolution trends (2000-2017)\n",
    "\n",
    "**Outputs**:\n",
    "- `paper_topics.csv` - Paper-topic assignments\n",
    "- `lda_model.pkl` - Trained LDA model\n",
    "- `tfidf_vectorizer.pkl` - Fitted vectorizer\n",
    "- `topic_distribution.png` - Bar chart of papers per topic\n",
    "- `topic_evolution.png` - Line chart showing top 5 topics over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
