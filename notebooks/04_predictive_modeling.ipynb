{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234fe2a7",
   "metadata": {},
   "source": [
    "# DBLP Citation Impact Prediction\n",
    "\n",
    "**Team Member:** Julio Amaya  \n",
    "**Task:** Predictive Modeling  \n",
    "**Date:** December 4, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds predictive models to forecast citation impact of research papers:\n",
    "\n",
    "1. Load paper data with features (metadata + topics)\n",
    "2. Engineer features for prediction\n",
    "3. Time-split data (pre-2010 train, post-2010 test)\n",
    "4. Train classifiers (Logistic Regression, XGBoost, Random Forest)\n",
    "5. Predict citation impact categories\n",
    "6. Evaluate models (F1, AUC, accuracy)\n",
    "7. Extract feature importance\n",
    "8. Analyze high/low impact papers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c99fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Detection and Configuration\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "\n",
    "# Try to import GPU-accelerated libraries\n",
    "try:\n",
    "    import cudf  # RAPIDS GPU DataFrame\n",
    "    import cuml  # RAPIDS GPU ML\n",
    "    from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "    from cuml.linear_model import LogisticRegression as cuLR\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"âœ“ RAPIDS cuDF and cuML available - GPU acceleration enabled\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"âš  RAPIDS not available - falling back to CPU with GPU-enabled XGBoost\")\n",
    "\n",
    "# XGBoost with GPU support\n",
    "import xgboost as xgb\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# Check GPU availability for XGBoost\n",
    "try:\n",
    "    # Test if GPU is available\n",
    "    xgb.DMatrix(np.array([[1, 2], [3, 4]]), label=np.array([0, 1]))\n",
    "    dtrain_test = xgb.DMatrix(np.array([[1, 2]]))\n",
    "    bst_test = xgb.train({'tree_method': 'gpu_hist', 'gpu_id': 0}, \n",
    "                         xgb.DMatrix(np.array([[1, 2], [3, 4]]), label=np.array([0, 1])), \n",
    "                         num_boost_round=1, verbose_eval=False)\n",
    "    print(\"âœ“ XGBoost GPU support confirmed\")\n",
    "    XGB_GPU = True\n",
    "except Exception as e:\n",
    "    print(f\"âš  XGBoost GPU not available: {e}\")\n",
    "    print(\"  Falling back to CPU tree_method\")\n",
    "    XGB_GPU = False\n",
    "\n",
    "# Standard libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\nGPU Configuration:\")\n",
    "print(f\"  RAPIDS GPU: {GPU_AVAILABLE}\")\n",
    "print(f\"  XGBoost GPU: {XGB_GPU}\")\n",
    "print(f\"\\nNotebook initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d453a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths\n",
    "BASE_DIR = Path('/Users/julio/Library/CloudStorage/OneDrive-UniversityOfHouston/0. Fall 2025/COSC3337/GitProject/COSC-3337-Project')\n",
    "DATA_DIR = BASE_DIR / 'data' / 'parquet'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "FIGURES_DIR = BASE_DIR / 'figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Working directories:\")\n",
    "print(f\"  Data: {DATA_DIR}\")\n",
    "print(f\"  Models: {MODELS_DIR}\")\n",
    "print(f\"  Results: {RESULTS_DIR}\")\n",
    "print(f\"  Figures: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Papers Data with Topics\n",
    "print(\"Loading papers with topics...\")\n",
    "papers_df = pd.read_csv(MODELS_DIR / 'papers_with_topics.csv')\n",
    "\n",
    "print(f\"\\nDataset shape: {papers_df.shape}\")\n",
    "print(f\"Columns: {papers_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(papers_df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(f\"  Total papers: {len(papers_df):,}\")\n",
    "print(f\"  Date range: {papers_df['year'].min()} - {papers_df['year'].max()}\")\n",
    "print(f\"  Citation range: {papers_df['n_citation'].min()} - {papers_df['n_citation'].max()}\")\n",
    "print(f\"  Median citations: {papers_df['n_citation'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"Engineering features for citation impact prediction...\")\n",
    "\n",
    "# Calculate citations per year (normalize by paper age)\n",
    "current_year = 2025\n",
    "papers_df['paper_age'] = current_year - papers_df['year']\n",
    "papers_df['citations_per_year'] = papers_df['n_citation'] / papers_df['paper_age'].replace(0, 1)\n",
    "\n",
    "# Create binary target: high impact (above median) vs low impact\n",
    "median_citations = papers_df['citations_per_year'].median()\n",
    "papers_df['high_impact'] = (papers_df['citations_per_year'] > median_citations).astype(int)\n",
    "\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(papers_df['high_impact'].value_counts())\n",
    "print(f\"\\nMedian citations per year: {median_citations:.2f}\")\n",
    "\n",
    "# Feature selection - prepare features for modeling\n",
    "feature_cols = []\n",
    "\n",
    "# Numeric features\n",
    "numeric_features = ['year', 'n_citation', 'paper_age']\n",
    "feature_cols.extend(numeric_features)\n",
    "\n",
    "# Venue encoding (if available)\n",
    "if 'venue' in papers_df.columns:\n",
    "    print(\"\\nEncoding venue feature...\")\n",
    "    # Use top 100 venues, group others as 'Other'\n",
    "    top_venues = papers_df['venue'].value_counts().head(100).index\n",
    "    papers_df['venue_encoded'] = papers_df['venue'].apply(\n",
    "        lambda x: x if x in top_venues else 'Other'\n",
    "    )\n",
    "    venue_encoder = LabelEncoder()\n",
    "    papers_df['venue_id'] = venue_encoder.fit_transform(papers_df['venue_encoded'])\n",
    "    feature_cols.append('venue_id')\n",
    "    print(f\"  Venues encoded: {len(venue_encoder.classes_)} unique venues\")\n",
    "\n",
    "# Topic features (if available)\n",
    "topic_cols = [col for col in papers_df.columns if col.startswith('topic_')]\n",
    "if topic_cols:\n",
    "    print(f\"\\nFound {len(topic_cols)} topic features\")\n",
    "    feature_cols.extend(topic_cols)\n",
    "\n",
    "# Reference count (if available)\n",
    "if 'references' in papers_df.columns:\n",
    "    papers_df['n_references'] = papers_df['references'].apply(\n",
    "        lambda x: len(eval(x)) if isinstance(x, str) and x != '[]' else 0\n",
    "    )\n",
    "    feature_cols.append('n_references')\n",
    "    print(f\"\\nAdded reference count feature\")\n",
    "\n",
    "print(f\"\\nTotal features for modeling: {len(feature_cols)}\")\n",
    "print(f\"Feature list: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce30fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Modeling with GPU Optimization\n",
    "print(\"Preparing data for GPU-accelerated modeling...\")\n",
    "\n",
    "# Remove rows with missing values\n",
    "papers_clean = papers_df.dropna(subset=feature_cols + ['high_impact'])\n",
    "print(f\"Clean dataset size: {len(papers_clean):,} papers\")\n",
    "\n",
    "# Time-based split: Train on pre-2010, Test on 2010+\n",
    "train_data = papers_clean[papers_clean['year'] < 2010].copy()\n",
    "test_data = papers_clean[papers_clean['year'] >= 2010].copy()\n",
    "\n",
    "print(f\"\\nTime-based split:\")\n",
    "print(f\"  Training set (pre-2010): {len(train_data):,} papers\")\n",
    "print(f\"  Test set (2010+): {len(test_data):,} papers\")\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data['high_impact'].values\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data['high_impact'].values\n",
    "\n",
    "print(f\"\\nFeature matrix shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "\n",
    "# Feature Scaling (important for GPU models)\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler for later use\n",
    "joblib.dump(scaler, MODELS_DIR / 'feature_scaler.pkl')\n",
    "print(f\"âœ“ Scaler saved to {MODELS_DIR / 'feature_scaler.pkl'}\")\n",
    "\n",
    "# Convert to GPU DataFrames if RAPIDS is available\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"\\nðŸš€ Converting to GPU DataFrames (cuDF)...\")\n",
    "    X_train_gpu = cudf.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "    X_test_gpu = cudf.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "    y_train_gpu = cudf.Series(y_train)\n",
    "    y_test_gpu = cudf.Series(y_test)\n",
    "    print(\"âœ“ Data transferred to GPU memory\")\n",
    "else:\n",
    "    print(\"\\nðŸ’» Using CPU arrays (GPU conversion skipped)\")\n",
    "    X_train_gpu = X_train_scaled\n",
    "    X_test_gpu = X_test_scaled\n",
    "    y_train_gpu = y_train\n",
    "    y_test_gpu = y_test\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e504a4",
   "metadata": {},
   "source": [
    "## Model Training with GPU Acceleration\n",
    "\n",
    "Train multiple classifiers optimized for GPU:\n",
    "1. **XGBoost with GPU**: `tree_method='gpu_hist'` for GPU-accelerated gradient boosting\n",
    "2. **RAPIDS cuML Models**: GPU-native Random Forest and Logistic Regression (if available)\n",
    "3. **Fallback CPU Models**: Standard scikit-learn models if GPU unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: XGBoost with GPU Acceleration\n",
    "print(\"=\"*60)\n",
    "print(\"Training XGBoost Classifier (GPU-Optimized)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configure XGBoost parameters for GPU\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "# Add GPU parameters if available\n",
    "if XGB_GPU:\n",
    "    xgb_params['tree_method'] = 'gpu_hist'\n",
    "    xgb_params['gpu_id'] = 0\n",
    "    xgb_params['predictor'] = 'gpu_predictor'\n",
    "    print(\"âœ“ GPU acceleration enabled (tree_method='gpu_hist')\")\n",
    "else:\n",
    "    xgb_params['tree_method'] = 'hist'  # Fast CPU histogram method\n",
    "    print(\"âš  Using CPU histogram method\")\n",
    "\n",
    "# Train model\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(X_train_scaled, y_train, \n",
    "              eval_set=[(X_test_scaled, y_test)],\n",
    "              verbose=False)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nâ± Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
    "xgb_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"\\nXGBoost Performance:\")\n",
    "print(f\"  F1 Score: {xgb_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {xgb_auc:.4f}\")\n",
    "print(f\"  Accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_model, MODELS_DIR / 'xgb_citation_predictor.pkl')\n",
    "print(f\"\\nâœ“ Model saved to {MODELS_DIR / 'xgb_citation_predictor.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Random Forest (GPU or CPU)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Random Forest Classifier\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"âœ“ Using RAPIDS cuML Random Forest (GPU)\")\n",
    "    rf_model = cuRF(\n",
    "        n_estimators=100,\n",
    "        max_depth=16,\n",
    "        max_features='sqrt',\n",
    "        n_bins=128,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train_gpu, y_train_gpu)\n",
    "    y_pred_rf = rf_model.predict(X_test_gpu).to_numpy()\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test_gpu)[:, 1].to_numpy()\n",
    "else:\n",
    "    print(\"âš  Using scikit-learn Random Forest (CPU)\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=16,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all CPU cores\n",
    "    )\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nâ± Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluation\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "rf_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest Performance:\")\n",
    "print(f\"  F1 Score: {rf_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {rf_auc:.4f}\")\n",
    "print(f\"  Accuracy: {rf_acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, MODELS_DIR / 'rf_citation_predictor.pkl')\n",
    "print(f\"\\nâœ“ Model saved to {MODELS_DIR / 'rf_citation_predictor.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16be11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Logistic Regression (GPU or CPU)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Logistic Regression Classifier\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"âœ“ Using RAPIDS cuML Logistic Regression (GPU)\")\n",
    "    lr_model = cuLR(\n",
    "        penalty='l2',\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        solver='qn'  # Quasi-Newton solver\n",
    "    )\n",
    "    lr_model.fit(X_train_gpu, y_train_gpu)\n",
    "    y_pred_lr = lr_model.predict(X_test_gpu).to_numpy()\n",
    "    y_pred_proba_lr = lr_model.predict_proba(X_test_gpu)[:, 1].to_numpy()\n",
    "else:\n",
    "    print(\"âš  Using scikit-learn Logistic Regression (CPU)\")\n",
    "    lr_model = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        solver='lbfgs',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "    y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nâ± Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluation\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"\\nLogistic Regression Performance:\")\n",
    "print(f\"  F1 Score: {lr_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {lr_auc:.4f}\")\n",
    "print(f\"  Accuracy: {lr_acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(lr_model, MODELS_DIR / 'lr_citation_predictor.pkl')\n",
    "print(f\"\\nâœ“ Model saved to {MODELS_DIR / 'lr_citation_predictor.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12287dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['XGBoost (GPU)', 'Random Forest', 'Logistic Regression'],\n",
    "    'F1 Score': [xgb_f1, rf_f1, lr_f1],\n",
    "    'AUC-ROC': [xgb_auc, rf_auc, lr_auc],\n",
    "    'Accuracy': [xgb_acc, rf_acc, lr_acc]\n",
    "})\n",
    "\n",
    "results = results.sort_values('F1 Score', ascending=False)\n",
    "print(\"\\n\", results.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "results.to_csv(RESULTS_DIR / 'model_comparison.csv', index=False)\n",
    "print(f\"\\nâœ“ Results saved to {RESULTS_DIR / 'model_comparison.csv'}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['F1 Score', 'AUC-ROC', 'Accuracy']\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx]\n",
    "    bars = ax.barh(results['Model'], results[metric], color=color, alpha=0.7)\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim([0.5, 1.0])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', \n",
    "                ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Figure saved to {FIGURES_DIR / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91aede0",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Extract feature importance from the best performing model to understand which factors most influence citation impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07081d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature Importance from XGBoost (typically the best model)\n",
    "print(\"Extracting feature importance from XGBoost model...\")\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(RESULTS_DIR / 'feature_importance.csv', index=False)\n",
    "print(f\"\\nâœ“ Feature importance saved to {RESULTS_DIR / 'feature_importance.csv'}\")\n",
    "\n",
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], \n",
    "         color='steelblue', alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 15 Most Important Features for Citation Impact Prediction', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Figure saved to {FIGURES_DIR / 'feature_importance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc83d1b",
   "metadata": {},
   "source": [
    "## Detailed Model Evaluation\n",
    "\n",
    "Generate confusion matrices and classification reports for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(\"=\"*60)\n",
    "print(\"XGBOOST MODEL EVALUATION DETAILS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, \n",
    "                          target_names=['Low Impact', 'High Impact']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Low Impact', 'High Impact'],\n",
    "            yticklabels=['Low Impact', 'High Impact'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix - XGBoost Citation Impact Predictor', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Confusion matrix saved to {FIGURES_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c86b4",
   "metadata": {},
   "source": [
    "## Case Studies: High and Low Impact Papers\n",
    "\n",
    "Analyze correctly predicted high and low impact papers to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5833765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Case Studies\n",
    "print(\"Analyzing correctly predicted papers...\")\n",
    "\n",
    "# Add predictions to test data\n",
    "test_results = test_data.copy()\n",
    "test_results['predicted_impact'] = y_pred_xgb\n",
    "test_results['prediction_confidence'] = y_pred_proba_xgb\n",
    "\n",
    "# Correctly predicted high impact papers\n",
    "correct_high = test_results[\n",
    "    (test_results['high_impact'] == 1) & \n",
    "    (test_results['predicted_impact'] == 1)\n",
    "].sort_values('prediction_confidence', ascending=False)\n",
    "\n",
    "print(f\"\\nCorrectly Predicted HIGH IMPACT Papers: {len(correct_high):,}\")\n",
    "print(\"\\nTop 10 High Impact Cases (highest confidence):\")\n",
    "high_impact_display = correct_high.head(10)[\n",
    "    ['id', 'title', 'year', 'n_citation', 'citations_per_year', 'prediction_confidence']\n",
    "].copy()\n",
    "print(high_impact_display.to_string(index=False))\n",
    "\n",
    "# Save high impact cases\n",
    "correct_high.head(50).to_csv(RESULTS_DIR / 'high_impact_cases.csv', index=False)\n",
    "\n",
    "# Correctly predicted low impact papers\n",
    "correct_low = test_results[\n",
    "    (test_results['high_impact'] == 0) & \n",
    "    (test_results['predicted_impact'] == 0)\n",
    "].sort_values('prediction_confidence', ascending=True)\n",
    "\n",
    "print(f\"\\n\\nCorrectly Predicted LOW IMPACT Papers: {len(correct_low):,}\")\n",
    "print(\"\\nTop 10 Low Impact Cases (highest confidence):\")\n",
    "low_impact_display = correct_low.head(10)[\n",
    "    ['id', 'title', 'year', 'n_citation', 'citations_per_year', 'prediction_confidence']\n",
    "].copy()\n",
    "print(low_impact_display.to_string(index=False))\n",
    "\n",
    "# Save low impact cases\n",
    "correct_low.head(50).to_csv(RESULTS_DIR / 'low_impact_cases.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Case studies saved to {RESULTS_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd78d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Confidence Distribution\n",
    "print(\"Analyzing prediction confidence distribution...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# High impact predictions\n",
    "axes[0].hist(test_results[test_results['high_impact'] == 1]['prediction_confidence'], \n",
    "             bins=30, color='green', alpha=0.6, edgecolor='black')\n",
    "axes[0].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('High Impact Papers - Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Low impact predictions\n",
    "axes[1].hist(test_results[test_results['high_impact'] == 0]['prediction_confidence'], \n",
    "             bins=30, color='orange', alpha=0.6, edgecolor='black')\n",
    "axes[1].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "axes[1].set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Low Impact Papers - Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'prediction_confidence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Confidence distribution saved to {FIGURES_DIR / 'prediction_confidence.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd31c99",
   "metadata": {},
   "source": [
    "## GPU Performance Benchmarking\n",
    "\n",
    "Compare GPU vs CPU performance to quantify acceleration benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU vs CPU Benchmark\n",
    "print(\"=\"*60)\n",
    "print(\"GPU vs CPU PERFORMANCE BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "# Benchmark XGBoost GPU vs CPU\n",
    "print(\"\\nBenchmarking XGBoost...\")\n",
    "\n",
    "# GPU version\n",
    "if XGB_GPU:\n",
    "    gpu_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'gpu_id': 0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    xgb_gpu = xgb.XGBClassifier(**gpu_params)\n",
    "    xgb_gpu.fit(X_train_scaled, y_train, verbose=False)\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  GPU training time: {gpu_time:.2f} seconds\")\n",
    "    benchmark_results.append({'Model': 'XGBoost', 'Device': 'GPU', 'Time (s)': gpu_time})\n",
    "else:\n",
    "    gpu_time = None\n",
    "    print(\"  GPU not available\")\n",
    "\n",
    "# CPU version\n",
    "cpu_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_cpu = xgb.XGBClassifier(**cpu_params)\n",
    "xgb_cpu.fit(X_train_scaled, y_train, verbose=False)\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "print(f\"  CPU training time: {cpu_time:.2f} seconds\")\n",
    "benchmark_results.append({'Model': 'XGBoost', 'Device': 'CPU', 'Time (s)': cpu_time})\n",
    "\n",
    "if gpu_time:\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"  Speedup: {speedup:.2f}x faster on GPU\")\n",
    "\n",
    "# Benchmark Random Forest (if RAPIDS available)\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"\\nBenchmarking Random Forest (RAPIDS GPU vs sklearn CPU)...\")\n",
    "    \n",
    "    # GPU version\n",
    "    start_time = time.time()\n",
    "    rf_gpu = cuRF(n_estimators=100, max_depth=16, random_state=42)\n",
    "    rf_gpu.fit(X_train_gpu, y_train_gpu)\n",
    "    gpu_rf_time = time.time() - start_time\n",
    "    print(f\"  GPU training time: {gpu_rf_time:.2f} seconds\")\n",
    "    benchmark_results.append({'Model': 'Random Forest', 'Device': 'GPU (RAPIDS)', 'Time (s)': gpu_rf_time})\n",
    "    \n",
    "    # CPU version\n",
    "    start_time = time.time()\n",
    "    rf_cpu = RandomForestClassifier(n_estimators=100, max_depth=16, random_state=42, n_jobs=-1)\n",
    "    rf_cpu.fit(X_train_scaled, y_train)\n",
    "    cpu_rf_time = time.time() - start_time\n",
    "    print(f\"  CPU training time: {cpu_rf_time:.2f} seconds\")\n",
    "    benchmark_results.append({'Model': 'Random Forest', 'Device': 'CPU (sklearn)', 'Time (s)': cpu_rf_time})\n",
    "    \n",
    "    speedup_rf = cpu_rf_time / gpu_rf_time\n",
    "    print(f\"  Speedup: {speedup_rf:.2f}x faster on GPU\")\n",
    "\n",
    "# Display benchmark results\n",
    "if benchmark_results:\n",
    "    benchmark_df = pd.DataFrame(benchmark_results)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARK SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(benchmark_df.to_string(index=False))\n",
    "    \n",
    "    # Save benchmark\n",
    "    benchmark_df.to_csv(RESULTS_DIR / 'gpu_benchmark.csv', index=False)\n",
    "    print(f\"\\nâœ“ Benchmark results saved to {RESULTS_DIR / 'gpu_benchmark.csv'}\")\n",
    "    \n",
    "    # Visualize if we have GPU results\n",
    "    if len(benchmark_df) > 2:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        models = benchmark_df['Model'].unique()\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        gpu_times = [benchmark_df[(benchmark_df['Model'] == m) & (benchmark_df['Device'].str.contains('GPU'))]['Time (s)'].values[0] \n",
    "                     if len(benchmark_df[(benchmark_df['Model'] == m) & (benchmark_df['Device'].str.contains('GPU'))]) > 0 else 0 \n",
    "                     for m in models]\n",
    "        cpu_times = [benchmark_df[(benchmark_df['Model'] == m) & (benchmark_df['Device'].str.contains('CPU'))]['Time (s)'].values[0] \n",
    "                     for m in models]\n",
    "        \n",
    "        ax.bar(x - width/2, gpu_times, width, label='GPU', color='green', alpha=0.7)\n",
    "        ax.bar(x + width/2, cpu_times, width, label='CPU', color='orange', alpha=0.7)\n",
    "        \n",
    "        ax.set_ylabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('GPU vs CPU Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'gpu_cpu_benchmark.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"âœ“ Benchmark figure saved to {FIGURES_DIR / 'gpu_cpu_benchmark.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74707f",
   "metadata": {},
   "source": [
    "## Summary and Results\n",
    "\n",
    "This GPU-optimized notebook successfully completed:\n",
    "\n",
    "### GPU Optimization Features:\n",
    "1. **XGBoost GPU Acceleration**: Used `tree_method='gpu_hist'` for GPU-accelerated gradient boosting\n",
    "2. **RAPIDS cuML Integration**: Attempted to use GPU-native Random Forest and Logistic Regression (if RAPIDS available)\n",
    "3. **Automatic Fallback**: Gracefully falls back to optimized CPU implementations if GPU unavailable\n",
    "4. **Performance Benchmarking**: Compared GPU vs CPU training times\n",
    "\n",
    "### Model Performance:\n",
    "- Trained 3 classifiers: XGBoost (GPU), Random Forest, and Logistic Regression\n",
    "- Time-based validation: Pre-2010 training, 2010+ testing\n",
    "- Comprehensive evaluation: F1, AUC-ROC, Accuracy metrics\n",
    "- Feature importance analysis for interpretability\n",
    "\n",
    "### Outputs Generated:\n",
    "- **Models**: Saved to `models/` directory\n",
    "- **Results**: Model comparisons, feature importance, case studies in `results/`\n",
    "- **Figures**: Visualizations in `figures/`\n",
    "\n",
    "### GPU Acceleration Benefits:\n",
    "- XGBoost: Significant speedup with `gpu_hist` tree method\n",
    "- RAPIDS cuML: Native GPU operations for entire ML pipeline (if available)\n",
    "- Scalable to larger datasets with minimal code changes\n",
    "\n",
    "### Installation Notes:\n",
    "For full GPU acceleration, install RAPIDS:\n",
    "```bash\n",
    "# For CUDA 11.x\n",
    "conda install -c rapidsai -c conda-forge -c nvidia rapids=23.12 python=3.11 cudatoolkit=11.8\n",
    "\n",
    "# Or for CUDA 12.x\n",
    "conda install -c rapidsai -c conda-forge -c nvidia rapids=23.12 python=3.11 cudatoolkit=12.0\n",
    "```\n",
    "\n",
    "The notebook works without RAPIDS, using XGBoost GPU support and CPU fallbacks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
