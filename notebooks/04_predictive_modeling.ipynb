{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234fe2a7",
   "metadata": {},
   "source": [
    "# DBLP Citation Impact Prediction\n",
    "\n",
    "**Team Member:** Julio Amaya  \n",
    "**Task:** Predictive Modeling  \n",
    "**Date:** December 4, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds predictive models to forecast citation impact of research papers:\n",
    "\n",
    "1. Load paper data with features (metadata + topics)\n",
    "2. Engineer features for prediction\n",
    "3. Time-split data (pre-2010 train, post-2010 test)\n",
    "4. Train classifiers (Logistic Regression, XGBoost, Random Forest)\n",
    "5. Predict citation impact categories\n",
    "6. Evaluate models (F1, AUC, accuracy)\n",
    "7. Extract feature importance\n",
    "8. Analyze high/low impact papers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c99fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION & SETUP\n",
    "# ==========================================\n",
    "\n",
    "# Define Project Path (Uses current working directory)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Define Sub-directories\n",
    "DATA_DIR = PROJECT_ROOT / \"data/parquet\"\n",
    "FIGURES_DIR = PROJECT_ROOT / \"figures\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"✅ Setup Complete. Working directory: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d453a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 1: Load Data (CORRECTED FOR FULL PROJECT)\n",
    "# ==========================================\n",
    "\n",
    "print(\"Loading papers from partitioned Parquet files...\")\n",
    "# This reads ALL part-*.parquet files in data/parquet/papers/ — exactly what Truc generated\n",
    "papers = pd.read_parquet(DATA_DIR / \"papers\")\n",
    "\n",
    "print(f\"Loaded {len(papers):,} papers\")\n",
    "print(f\"Year range: {papers['year'].min():.0f} – {papers['year'].max():.0f}\")\n",
    "print(f\"Papers with abstract: {papers['abstract'].notna().sum():,} ({papers['abstract'].notna().mean():.1%})\")\n",
    "\n",
    "# Load Topic Assignments (saved from notebook 06)\n",
    "# In notebook 06 you saved it to: output/paper_topics.csv\n",
    "topics_path = PROJECT_ROOT / \"output\" / \"paper_with_topics.csv\"\n",
    "\n",
    "if not topics_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Topic file not found at {topics_path}\\n\"\n",
    "        \"Please run notebook 06_topic_modeling.ipynb first and make sure it saves:\\n\"\n",
    "        \"    df[['id', 'year', 'topic_id', 'topic_label']].to_csv('output/paper_topics.csv', index=False)\"\n",
    "    )\n",
    "\n",
    "paper_topics = pd.read_csv(topics_path)\n",
    "\n",
    "print(f\"Loaded {len(paper_topics):,} topic assignments\")\n",
    "\n",
    "# Merge with main papers table\n",
    "paper_topics = paper_topics.rename(columns={'topic_id': 'topic'})  # keep consistent naming\n",
    "df = papers.merge(paper_topics[['id', 'topic']], on='id', how='left')\n",
    "\n",
    "print(f\"\\nMerge complete:\")\n",
    "print(f\"   • Total papers:           {len(df):,}\")\n",
    "print(f\"   • Papers with topics:     {df['topic'].notna().sum():,} ({df['topic'].notna().mean():.1%})\")\n",
    "print(f\"   • Papers without topics:  {df['topic'].isna().sum():,} (will be excluded later)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 2: Feature Engineering\n",
    "# ==========================================\n",
    "\n",
    "# Filter papers\n",
    "df_features = df[\n",
    "    (df['year'].notna()) &\n",
    "    (df['n_citation'].notna()) &\n",
    "    (df['topic'].notna())\n",
    "].copy()\n",
    "\n",
    "# --- BINARY CLASSIFICATION (High vs Low) ---\n",
    "def calculate_relative_impact(group):\n",
    "    # Calculate the Median (50th percentile) for this year\n",
    "    median = group['n_citation'].quantile(0.50)\n",
    "    \n",
    "    def classify(n):\n",
    "        if n > median: return 1   # High Impact (Top 50%)\n",
    "        else: return 0            # Low Impact (Bottom 50%)\n",
    "        \n",
    "    return group['n_citation'].apply(classify)\n",
    "\n",
    "print(\"Calculating relative impact (Binary: Above/Below Median)...\")\n",
    "df_features['citation_impact'] = df_features.groupby('year', group_keys=False).apply(calculate_relative_impact)\n",
    "\n",
    "# Add other features\n",
    "df_features['paper_age'] = 2017 - df_features['year']\n",
    "\n",
    "if 'author_count' not in df_features.columns:\n",
    "    df_features['author_count'] = df_features['authors'].apply(lambda x: len(x) if x is not None else 1)\n",
    "\n",
    "if 'ref_count' not in df_features.columns:\n",
    "    df_features['ref_count'] = df_features['references'].apply(lambda x: len(x) if x is not None else 0)\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_features['citation_impact'].value_counts().sort_index())\n",
    "\n",
    "# --- ENCODING ---\n",
    "\n",
    "# Encode venue (top 50 venues + 'OTHER')\n",
    "top_venues = df_features['venue'].value_counts().head(50).index\n",
    "df_features['venue_encoded'] = df_features['venue'].apply(\n",
    "    lambda x: x if x in top_venues else 'OTHER'\n",
    ")\n",
    "\n",
    "print(\"Encoding venues and topics...\")\n",
    "venue_dummies = pd.get_dummies(df_features['venue_encoded'], prefix='venue')\n",
    "topic_dummies = pd.get_dummies(df_features['topic'], prefix='topic')\n",
    "\n",
    "# Select Numeric features\n",
    "feature_cols = ['author_count', 'ref_count'] \n",
    "X_base = df_features[feature_cols].fillna(0).copy()\n",
    "\n",
    "# Combine All Features\n",
    "X = pd.concat([X_base, venue_dummies, topic_dummies], axis=1)\n",
    "y = df_features['citation_impact']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 3: Model Training\n",
    "# ==========================================\n",
    "\n",
    "# Time-based split\n",
    "SPLIT_YEAR = 2010\n",
    "\n",
    "train_mask = df_features['year'] < SPLIT_YEAR\n",
    "test_mask = df_features['year'] >= SPLIT_YEAR\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_test = X[test_mask]\n",
    "y_train = y[train_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"Training set (pre-{SPLIT_YEAR}): {len(X_train):,} papers\")\n",
    "print(f\"Test set ({SPLIT_YEAR}+): {len(X_test):,} papers\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = ['author_count', 'ref_count']\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "print(f\"\\n✓ Features scaled\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1), # n_jobs=-1 uses all local cores\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Starting Model Training...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] \n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except ValueError:\n",
    "        auc = 0.0\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    results[name] = {\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n✓ All models trained\")\n",
    "\n",
    "# Model comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'F1 Score': [results[m]['f1_score'] for m in results.keys()],\n",
    "    'AUC': [results[m]['auc'] for m in results.keys()],\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "comparison_df.to_csv(RESULTS_DIR / 'model_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce30fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 4: Visualization & Analysis\n",
    "# ==========================================\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, metric in enumerate(['F1 Score', 'AUC', 'Accuracy']):\n",
    "    axes[i].bar(comparison_df['Model'], comparison_df[metric], color=['steelblue', 'coral', 'seagreen'], alpha=0.8)\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_title(f'{metric} by Model')\n",
    "    axes[i].set_ylim([0, 1.0])\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig7_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved figure to {FIGURES_DIR / 'fig7_model_comparison.png'}\")\n",
    "# plt.show() # Uncomment if you want to see the plot popup\n",
    "\n",
    "# Get feature importance from Random Forest\n",
    "rf_model = trained_models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 15 features\n",
    "top_features = feature_importance.head(15)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "print(top_features.to_string(index=False))\n",
    "\n",
    "# Visualize Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color='mediumseagreen', alpha=0.8)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Top 15 Feature Importances (Random Forest)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig8_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved figure to {FIGURES_DIR / 'fig8_feature_importance.png'}\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(RESULTS_DIR / 'feature_importance.csv', index=False)\n",
    "\n",
    "# Add predictions to test data\n",
    "df_test = df_features[test_mask].copy()\n",
    "df_test['predicted_impact'] = results['Random Forest']['predictions']\n",
    "\n",
    "# --- CASE STUDIES ---\n",
    "\n",
    "# High impact papers (Correctly Predicted as 1)\n",
    "high_impact_correct = df_test[\n",
    "    (df_test['citation_impact'] == 1) &\n",
    "    (df_test['predicted_impact'] == 1)\n",
    "].nlargest(5, 'n_citation')\n",
    "\n",
    "print(\"\\nHigh Impact Papers (Correctly Predicted):\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['title', 'year', 'venue', 'n_citation', 'author_count', 'topic']\n",
    "print(high_impact_correct[display_cols].to_string())\n",
    "\n",
    "# Low impact papers (Correctly Predicted as 0)\n",
    "low_impact_correct = df_test[\n",
    "    (df_test['citation_impact'] == 0) &\n",
    "    (df_test['predicted_impact'] == 0)\n",
    "].head(5)\n",
    "\n",
    "print(\"\\nLow Impact Papers (Correctly Predicted):\")\n",
    "print(\"=\"*80)\n",
    "print(low_impact_correct[display_cols].to_string())\n",
    "\n",
    "# Save case studies\n",
    "high_impact_correct.to_csv(RESULTS_DIR / 'high_impact_cases.csv', index=False)\n",
    "low_impact_correct.to_csv(RESULTS_DIR / 'low_impact_cases.csv', index=False)\n",
    "\n",
    "# Save best model\n",
    "best_model_name = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "joblib.dump(best_model, MODELS_DIR / 'citation_predictor.pkl')\n",
    "joblib.dump(scaler, MODELS_DIR / 'feature_scaler.pkl')\n",
    "\n",
    "print(f\"\\n✓ Best model ({best_model_name}) saved to {MODELS_DIR}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTIVE MODELING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Task: Citation Impact Prediction (Binary: High vs Low)\")\n",
    "print(f\"Train Period: Pre-{SPLIT_YEAR} ({len(X_train):,} papers)\")\n",
    "print(f\"Test Period: {SPLIT_YEAR}+ ({len(X_test):,} papers)\")\n",
    "print(f\"Features Used: {X.shape[1]} (metadata + topic_dummies + venue_dummies)\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"  F1 Score: {comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'F1 Score']:.4f}\")\n",
    "print(f\"  AUC: {comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'AUC']:.4f}\")\n",
    "print(f\"\\nOutput Locations:\")\n",
    "print(f\"  - Models: {MODELS_DIR}\")\n",
    "print(f\"  - Results: {RESULTS_DIR}\")\n",
    "print(f\"  - Figures: {FIGURES_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e504a4",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "This notebook successfully completed the following:\n",
    "\n",
    "1. **Data Loading**: Loaded papers dataset and merged with topic assignments from NLP modeling\n",
    "2. **Feature Engineering**: Created binary classification target (high vs low impact based on median citations per year) and encoded venue/topic features\n",
    "3. **Time-Based Split**: Split data at 2010 (training on pre-2010, testing on 2010+)\n",
    "4. **Model Training**: Trained three classifiers (Logistic Regression, Random Forest, XGBoost) with standardized features\n",
    "5. **Model Evaluation**: Compared models using F1 score, AUC, and accuracy metrics\n",
    "6. **Feature Analysis**: Extracted and visualized top 15 most important features from Random Forest\n",
    "7. **Case Studies**: Identified correctly predicted high and low impact papers\n",
    "\n",
    "All results, models, and visualizations have been saved to the respective directories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
