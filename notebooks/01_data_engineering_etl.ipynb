{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da46a9a",
   "metadata": {},
   "source": [
    "# DBLP Data Engineering & ETL Pipeline\n",
    "\n",
    "**Team Member:** Truc Le  \n",
    "**Task:** Data Engineering & Infrastructure  \n",
    "**Date:** December 1, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the ETL (Extract, Transform, Load) pipeline for the DBLP research dataset:\n",
    "\n",
    "1. **Extract:** Ingest JSON shards from DBLP dataset\n",
    "2. **Transform:** Normalize schema, build citation and coauthor edges\n",
    "3. **Load:** Store cleaned data as Parquet files\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- ✅ Cleaned datasets (papers, authorships, citations, coauthorships)\n",
    "- ✅ Schema normalization and data quality checks\n",
    "- ✅ Citation network edges (directed)\n",
    "- ✅ Coauthor network edges (undirected)\n",
    "- ✅ Parquet storage with partitioning\n",
    "- ✅ Data dictionary and profiling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6734e3d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1199b04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cfc5c",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53ed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: /Users/julio/Library/CloudStorage/OneDrive-UniversityOfHouston/0. Fall 2025/COSC3337/GitProject/COSC-3337-Project/notebooks/../dblp-ref\n",
      "Output directory: /Users/julio/Library/CloudStorage/OneDrive-UniversityOfHouston/0. Fall 2025/COSC3337/GitProject/COSC-3337-Project/notebooks/../data/parquet\n",
      "Chunk size: 50,000 records\n",
      "Max records: ALL\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_DIR = Path(\"../dblp-ref\")\n",
    "OUTPUT_DIR = Path(\"../data/parquet\")\n",
    "CHUNK_SIZE = 50000  # Records per output part file\n",
    "MAX_RECORDS = 0  # Set to 0 for full dataset, or limit for testing\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for table in ['papers', 'authorships', 'citations', 'coauthorships']:\n",
    "    (OUTPUT_DIR / table).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {INPUT_DIR.absolute()}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE:,} records\")\n",
    "print(f\"Max records: {MAX_RECORDS:,}\" if MAX_RECORDS > 0 else \"Max records: ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358ee1e",
   "metadata": {},
   "source": [
    "## 3. Data Normalization Functions\n",
    "\n",
    "These functions handle schema normalization and data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2859cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "José García-López              → jose garcia-lopez\n",
      "JOHN SMITH                     → john smith\n",
      "Marie-Claire Dubois            → marie-claire dubois\n"
     ]
    }
   ],
   "source": [
    "def normalize_author(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize author names for deduplication:\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    - Collapse multiple spaces\n",
    "    - Remove Unicode accents\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        return None\n",
    "    \n",
    "    # Strip and lowercase\n",
    "    s = name.strip().lower()\n",
    "    \n",
    "    # Collapse spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    \n",
    "    # Remove accents using Unicode normalization\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Test the function\n",
    "test_names = [\"José García-López  \", \"JOHN SMITH\", \"Marie-Claire Dubois\"]\n",
    "for name in test_names:\n",
    "    print(f\"{name:30} → {normalize_author(name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca8771",
   "metadata": {},
   "source": [
    "## 4. ETL Processing Function\n",
    "\n",
    "This function processes batches of JSON records and generates four tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_records(records: List[Dict]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a batch of JSON records into four tables:\n",
    "    1. papers - publication metadata\n",
    "    2. authorships - author-paper relationships\n",
    "    3. citations - citation edges (directed)\n",
    "    4. coauthorships - coauthor edges (undirected)\n",
    "    \"\"\"\n",
    "    papers_rows = []\n",
    "    authorships_rows = []\n",
    "    citations_rows = []\n",
    "    coauthor_rows = []\n",
    "\n",
    "    for rec in records:\n",
    "        pid = rec.get(\"id\")\n",
    "        if not pid:\n",
    "            continue\n",
    "        \n",
    "        # Extract fields\n",
    "        title = rec.get(\"title\")\n",
    "        venue = rec.get(\"venue\")\n",
    "        year = rec.get(\"year\")\n",
    "        abstract = rec.get(\"abstract\")\n",
    "        n_citation = rec.get(\"n_citation\")\n",
    "        refs = rec.get(\"references\") or []\n",
    "        authors = rec.get(\"authors\") or []\n",
    "\n",
    "        # Compute derived fields\n",
    "        abstract_len = len(abstract) if isinstance(abstract, str) else None\n",
    "        ref_count = len(refs) if isinstance(refs, list) else 0\n",
    "        author_count = len(authors) if isinstance(authors, list) else 0\n",
    "\n",
    "        # Papers table\n",
    "        papers_rows.append({\n",
    "            \"id\": pid,\n",
    "            \"title\": title,\n",
    "            \"venue\": venue if venue is not None else None,\n",
    "            \"year\": int(year) if isinstance(year, int) or (isinstance(year, str) and year.isdigit()) else None,\n",
    "            \"n_citation\": int(n_citation) if isinstance(n_citation, int) or (isinstance(n_citation, str) and n_citation.isdigit()) else None,\n",
    "            \"abstract\": abstract if isinstance(abstract, str) else None,\n",
    "            \"abstract_len\": abstract_len,\n",
    "            \"ref_count\": ref_count,\n",
    "            \"author_count\": author_count,\n",
    "        })\n",
    "\n",
    "        # Authorships table + build coauthor edges\n",
    "        normalized_authors = []\n",
    "        for pos, a in enumerate(authors):\n",
    "            norm_name = normalize_author(a) if isinstance(a, str) else None\n",
    "            authorships_rows.append({\n",
    "                \"paper_id\": pid,\n",
    "                \"author_name\": a,\n",
    "                \"author_position\": pos,\n",
    "                \"author_norm\": norm_name,\n",
    "            })\n",
    "            if norm_name:\n",
    "                normalized_authors.append(norm_name)\n",
    "\n",
    "        # Coauthor edges (undirected, all pairs)\n",
    "        for i in range(len(normalized_authors)):\n",
    "            for j in range(i + 1, len(normalized_authors)):\n",
    "                a1, a2 = normalized_authors[i], normalized_authors[j]\n",
    "                # Sort for consistency (undirected)\n",
    "                if a1 > a2:\n",
    "                    a1, a2 = a2, a1\n",
    "                coauthor_rows.append({\n",
    "                    \"author1_norm\": a1,\n",
    "                    \"author2_norm\": a2,\n",
    "                    \"paper_id\": pid,\n",
    "                    \"year\": papers_rows[-1][\"year\"],\n",
    "                    \"venue\": venue if venue is not None else None,\n",
    "                })\n",
    "\n",
    "        # Citations table (directed edges)\n",
    "        for dst in refs:\n",
    "            if dst:\n",
    "                citations_rows.append({\n",
    "                    \"src_id\": pid,\n",
    "                    \"dst_id\": dst,\n",
    "                    \"src_year\": papers_rows[-1][\"year\"],\n",
    "                    \"src_venue\": venue if venue is not None else None,\n",
    "                })\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame.from_records(papers_rows),\n",
    "        pd.DataFrame.from_records(authorships_rows),\n",
    "        pd.DataFrame.from_records(citations_rows),\n",
    "        pd.DataFrame.from_records(coauthor_rows)\n",
    "    )\n",
    "\n",
    "print(\"✓ Processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb45c8",
   "metadata": {},
   "source": [
    "## 5. Run ETL Pipeline\n",
    "\n",
    "Load JSON shards and process them in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10bd279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 input files:\n",
      "  - dblp-ref-0.json\n",
      "  - dblp-ref-1.json\n",
      "  - dblp-ref-2.json\n",
      "  - dblp-ref-3.json\n",
      "\n",
      "Processing with chunk size: 50,000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dblp-ref-0.json: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dblp-ref-0.json: 1000000it [00:29, 34117.97it/s]\n",
      "Processing dblp-ref-1.json: 1000000it [00:31, 31809.71it/s]\n",
      "Processing dblp-ref-2.json: 1000000it [00:35, 28139.72it/s]\n",
      "Processing dblp-ref-3.json: 79007it [00:01, 40196.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ ETL Complete!\n",
      "Total records processed: 3,079,007\n",
      "\n",
      "Part files created:\n",
      "  papers: 62 parts\n",
      "  authorships: 62 parts\n",
      "  citations: 62 parts\n",
      "  coauthorships: 62 parts\n"
     ]
    }
   ],
   "source": [
    "# Find input files\n",
    "input_files = sorted(INPUT_DIR.glob(\"dblp-ref-*.json\"))\n",
    "print(f\"Found {len(input_files)} input files:\")\n",
    "for f in input_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Initialize counters\n",
    "total_records = 0\n",
    "part_num = {'papers': 0, 'authorships': 0, 'citations': 0, 'coauthorships': 0}\n",
    "buffer = []\n",
    "\n",
    "print(f\"\\nProcessing with chunk size: {CHUNK_SIZE:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Process files\n",
    "for file_path in input_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as fh:\n",
    "        for line in tqdm(fh, desc=f\"Processing {file_path.name}\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "                \n",
    "            buffer.append(rec)\n",
    "            total_records += 1\n",
    "            \n",
    "            # Process chunk\n",
    "            if len(buffer) >= CHUNK_SIZE:\n",
    "                papers_df, auth_df, cite_df, coauth_df = process_records(buffer)\n",
    "                \n",
    "                # Save to parquet\n",
    "                part_num['papers'] += 1\n",
    "                papers_df.to_parquet(OUTPUT_DIR / 'papers' / f'part-{part_num[\"papers\"]:05d}.parquet', index=False)\n",
    "                \n",
    "                if not auth_df.empty:\n",
    "                    part_num['authorships'] += 1\n",
    "                    auth_df.to_parquet(OUTPUT_DIR / 'authorships' / f'part-{part_num[\"authorships\"]:05d}.parquet', index=False)\n",
    "                \n",
    "                if not cite_df.empty:\n",
    "                    part_num['citations'] += 1\n",
    "                    cite_df.to_parquet(OUTPUT_DIR / 'citations' / f'part-{part_num[\"citations\"]:05d}.parquet', index=False)\n",
    "                \n",
    "                if not coauth_df.empty:\n",
    "                    part_num['coauthorships'] += 1\n",
    "                    coauth_df.to_parquet(OUTPUT_DIR / 'coauthorships' / f'part-{part_num[\"coauthorships\"]:05d}.parquet', index=False)\n",
    "                \n",
    "                buffer.clear()\n",
    "            \n",
    "            # Stop if max records reached\n",
    "            if MAX_RECORDS > 0 and total_records >= MAX_RECORDS:\n",
    "                break\n",
    "    \n",
    "    if MAX_RECORDS > 0 and total_records >= MAX_RECORDS:\n",
    "        break\n",
    "\n",
    "# Process remaining buffer\n",
    "if buffer:\n",
    "    papers_df, auth_df, cite_df, coauth_df = process_records(buffer)\n",
    "    \n",
    "    part_num['papers'] += 1\n",
    "    papers_df.to_parquet(OUTPUT_DIR / 'papers' / f'part-{part_num[\"papers\"]:05d}.parquet', index=False)\n",
    "    \n",
    "    if not auth_df.empty:\n",
    "        part_num['authorships'] += 1\n",
    "        auth_df.to_parquet(OUTPUT_DIR / 'authorships' / f'part-{part_num[\"authorships\"]:05d}.parquet', index=False)\n",
    "    \n",
    "    if not cite_df.empty:\n",
    "        part_num['citations'] += 1\n",
    "        cite_df.to_parquet(OUTPUT_DIR / 'citations' / f'part-{part_num[\"citations\"]:05d}.parquet', index=False)\n",
    "    \n",
    "    if not coauth_df.empty:\n",
    "        part_num['coauthorships'] += 1\n",
    "        coauth_df.to_parquet(OUTPUT_DIR / 'coauthorships' / f'part-{part_num[\"coauthorships\"]:05d}.parquet', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ ETL Complete!\")\n",
    "print(f\"Total records processed: {total_records:,}\")\n",
    "print(f\"\\nPart files created:\")\n",
    "for table, count in part_num.items():\n",
    "    print(f\"  {table}: {count} parts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8cb97",
   "metadata": {},
   "source": [
    "## 6. Verify Output and Data Quality\n",
    "\n",
    "Load the generated datasets and perform basic validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c161841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "============================================================\n",
      "Papers:          3,079,007 records\n",
      "Authorships:     9,476,165 records\n",
      "Citations:      25,166,994 records\n",
      "Coauthorships:  14,724,453 records\n",
      "\n",
      "Unique authors:  1,751,941\n",
      "Unique venues:       5,079\n",
      "\n",
      "============================================================\n",
      "Sample from Papers table:\n",
      "                                     id  \\\n",
      "0  00127ee2-cb05-48ce-bc49-9de556b93346   \n",
      "1  001c58d3-26ad-46b3-ab3a-c1e557d16821   \n",
      "2  001c8744-73c4-4b04-9364-22d31a10dbf1   \n",
      "\n",
      "                                               title  \\\n",
      "0  Preliminary Design of a Network Protocol Learn...   \n",
      "1  A methodology for the physically accurate visu...   \n",
      "2  Comparison of GARCH, Neural Network and Suppor...   \n",
      "\n",
      "                                               venue  year  n_citation  \\\n",
      "0  international conference on human-computer int...  2013           0   \n",
      "1            visual analytics science and technology  2011          50   \n",
      "2       pattern recognition and machine intelligence  2009          50   \n",
      "\n",
      "                                            abstract  abstract_len  ref_count  \\\n",
      "0  The purpose of this study is to develop a lear...         379.0          2   \n",
      "1  This paper describes the design and implementa...        1357.0         13   \n",
      "2  This article applied GARCH model instead AR or...         414.0          2   \n",
      "\n",
      "   author_count  \n",
      "0             8  \n",
      "1             2  \n",
      "2             4  \n"
     ]
    }
   ],
   "source": [
    "# Load all generated tables\n",
    "papers = pd.read_parquet(OUTPUT_DIR / 'papers')\n",
    "authorships = pd.read_parquet(OUTPUT_DIR / 'authorships')\n",
    "citations = pd.read_parquet(OUTPUT_DIR / 'citations')\n",
    "coauthorships = pd.read_parquet(OUTPUT_DIR / 'coauthorships')\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Papers:         {len(papers):>10,} records\")\n",
    "print(f\"Authorships:    {len(authorships):>10,} records\")\n",
    "print(f\"Citations:      {len(citations):>10,} records\")\n",
    "print(f\"Coauthorships:  {len(coauthorships):>10,} records\")\n",
    "print(f\"\\nUnique authors: {authorships['author_norm'].nunique():>10,}\")\n",
    "print(f\"Unique venues:  {papers['venue'].nunique():>10,}\")\n",
    "\n",
    "# Display sample from each table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample from Papers table:\")\n",
    "print(papers.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Checks:\n",
      "============================================================\n",
      "✓ Authorships referential integrity: True\n",
      "✓ Citations source integrity: True\n",
      "\n",
      "Missing Values:\n",
      "  Abstracts: 530,475 (17.2%)\n",
      "  Venues: 0 (0.0%)\n",
      "  Years: 0 (0.0%)\n",
      "\n",
      "Data Ranges:\n",
      "  Year range: 1936 - 2018\n",
      "  Citations per paper (mean): 35.22\n",
      "  Authors per paper (mean): 3.08\n",
      "  References per paper (mean): 8.17\n"
     ]
    }
   ],
   "source": [
    "# Quality checks\n",
    "print(\"\\nData Quality Checks:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Referential integrity (authorships → papers)\n",
    "auth_paper_ids = set(authorships['paper_id'])\n",
    "paper_ids = set(papers['id'])\n",
    "orphaned_auth = auth_paper_ids - paper_ids\n",
    "print(f\"✓ Authorships referential integrity: {len(orphaned_auth) == 0}\")\n",
    "if orphaned_auth:\n",
    "    print(f\"  Warning: {len(orphaned_auth)} orphaned authorships\")\n",
    "\n",
    "# Check 2: Citations source integrity\n",
    "cite_src_ids = set(citations['src_id'])\n",
    "orphaned_cite = cite_src_ids - paper_ids\n",
    "print(f\"✓ Citations source integrity: {len(orphaned_cite) == 0}\")\n",
    "if orphaned_cite:\n",
    "    print(f\"  Warning: {len(orphaned_cite)} orphaned citation sources\")\n",
    "\n",
    "# Check 3: Missing values\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(f\"  Abstracts: {papers['abstract'].isna().sum():,} ({papers['abstract'].isna().sum()/len(papers)*100:.1f}%)\")\n",
    "print(f\"  Venues: {papers['venue'].isna().sum():,} ({papers['venue'].isna().sum()/len(papers)*100:.1f}%)\")\n",
    "print(f\"  Years: {papers['year'].isna().sum():,} ({papers['year'].isna().sum()/len(papers)*100:.1f}%)\")\n",
    "\n",
    "# Check 4: Data ranges\n",
    "print(f\"\\nData Ranges:\")\n",
    "print(f\"  Year range: {papers['year'].min():.0f} - {papers['year'].max():.0f}\")\n",
    "print(f\"  Citations per paper (mean): {papers['n_citation'].mean():.2f}\")\n",
    "print(f\"  Authors per paper (mean): {papers['author_count'].mean():.2f}\")\n",
    "print(f\"  References per paper (mean): {papers['ref_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad059c3",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Deliverables Completed ✓\n",
    "\n",
    "1. **Cleaned Datasets** stored in `../data/parquet/`:\n",
    "   - `papers/` - Core publication metadata\n",
    "   - `authorships/` - Author-paper relationships with normalized names\n",
    "   - `citations/` - Directed citation network edges\n",
    "   - `coauthorships/` - Undirected collaboration network edges\n",
    "\n",
    "2. **Schema Normalization**:\n",
    "   - Type validation and conversion\n",
    "   - Author name normalization (Unicode, case-folding)\n",
    "   - Derived metrics (abstract_len, ref_count, author_count)\n",
    "\n",
    "3. **Network Edge Construction**:\n",
    "   - Citation edges: Directed, with temporal/venue metadata\n",
    "   - Coauthor edges: Undirected, sorted pairs for consistency\n",
    "\n",
    "### For Teammates\n",
    "\n",
    "**Next notebooks to complete:**\n",
    "- `02_data_profiling_analysis.ipynb` - Comprehensive profiling and visualization\n",
    "- `03_network_analysis_template.ipynb` - Network metrics and community detection\n",
    "- `04_topic_modeling.ipynb` - TF-IDF and topic extraction from abstracts\n",
    "- `05_visualization_report.ipynb` - Generate figures and tables for final report\n",
    "\n",
    "**Key files:**\n",
    "- Data dictionary: `../docs/data_dictionary.md`\n",
    "- Query utilities: `../scripts/query_utils.py`\n",
    "- See README.md for usage instructions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
