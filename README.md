# Mapping Knowledge: Collaboration, Topics, and Influence in the DBLP Citation Network

**Group 7**

## ðŸ“Œ Project Overview
This project aims to understand the evolution of computer science research using the DBLP dataset. We will analyze collaboration trends, rising research topics, and patterns of influence by building citation and co-authorship networks. The project involves a scalable ETL pipeline, network analysis, topic modeling, and predictive modeling.

## ðŸ‘¥ Team & Roles

| Member | Role | Focus Areas |
|--------|------|-------------|
| **Truc Le** | Data Engineering & Infrastructure | Ingestion, Schema Normalization, Parquet Storage, Edge Building (Citations/Co-authors) |
| **Ai Nhien To** | Networks & Metrics | Graph Construction, Centrality/Community Computation, Temporal Slicing, Influence Trajectories |
| **Julio Amaya** | NLP & Modeling | Text Cleaning, Topic Modeling (TF-IDF/LDA), Trend Analysis, Predictive Modeling (Citation Impact) |

## ðŸ“‚ Project Structure

```
COSC-3337-Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Raw DBLP JSON shards (not committed)
â”‚   â””â”€â”€ parquet/              # Cleaned Parquet datasets
â”‚       â”œâ”€â”€ papers/           # Core publication metadata
â”‚       â”œâ”€â”€ authorships/      # Author-paper relationships
â”‚       â”œâ”€â”€ citations/        # Citation network edges
â”‚       â””â”€â”€ coauthorships/    # Coauthor collaboration edges
â”œâ”€â”€ notebooks/                # Jupyter Notebooks for Analysis
â”‚   â”œâ”€â”€ 01_data_engineering_etl.ipynb      # ETL Pipeline (Truc)
â”‚   â”œâ”€â”€ 02_data_profiling_analysis.ipynb   # Data Profiling (Truc)
â”‚   â”œâ”€â”€ 03_network_analysis.ipynb          # Network metrics & graphs (Ai Nhien)
â”‚   â”œâ”€â”€ 04_predictive_modeling.ipynb       # Predictive modeling (Julio)
â”‚   â”œâ”€â”€ 05_anomaly_detection.ipynb         # Anomaly detection
â”‚   â””â”€â”€ 06_topic_modeling.ipynb            # Topic modeling (Julio)
â”œâ”€â”€ src/                      # Source Code Modules
â”‚   â”œâ”€â”€ etl/                  # Data Engineering (Truc)
â”‚   â”‚   â”œâ”€â”€ ingestion.py      # JSON streaming & parsing
â”‚   â”‚   â””â”€â”€ processing.py     # Cleaning & Parquet conversion
â”‚   â”œâ”€â”€ networks/             # Network Analysis (Ai Nhien)
â”‚   â”‚   â”œâ”€â”€ graph_builder.py  # NetworkX/igraph construction
â”‚   â”‚   â””â”€â”€ metrics.py        # Centrality & Community algorithms
â”‚   â”œâ”€â”€ nlp_modeling/         # NLP & ML (Julio)
â”‚   â”‚   â”œâ”€â”€ text_processing.py # TF-IDF, Cleaning
â”‚   â”‚   â””â”€â”€ models.py         # LDA, Classifiers (LogReg, XGBoost)
â”‚   â””â”€â”€ utils/                # Shared utilities
â”‚       â””â”€â”€ config.py         # Paths and constants
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_dictionary.md    # Comprehensive data documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Project Documentation
```

## ðŸš€ Getting Started

### 1. Environment Setup

**Prerequisites:**
- Python 3.13.1 or higher (required for all dependencies to work correctly)

Create a virtual environment and install dependencies:
```bash
python --version  # Verify you have Python 3.13.1+
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

If you encounter a `ModuleNotFoundError` for `networkx`, install it manually:
```bash
pip install networkx
```

### 2. Data Processing 
The ETL pipeline has been successfully executed. All cleaned datasets are available in `data/parquet/`:
- Papers metadata with 9 columns
- Authorships with normalized author names
- Citations network edges
- Coauthorships network edges

See `docs/data_dictionary.md` for detailed schema documentation.

### 3. Analysis Workflow
- **Data Profiling**: Review `notebooks/02_data_profiling_analysis.ipynb` for dataset statistics
- **Networks**: Use `notebooks/03_network_analysis.ipynb` for graph analysis (Ai Nhien)
- **NLP/ML**: Use `notebooks/04_predictive_modeling.ipynb` and `06_topic_modeling.ipynb` (Julio)

## Project Progress & Roadmap

### Phase 1: Data Engineering (Truc) 
- [x] Implement chunked stream-parsing for JSON shards
- [x] Clean data: Drop missing IDs, normalize venues, handle missing abstracts
- [x] Build citation network edges (directed graph)
- [x] Build coauthorship network edges (undirected graph)
- [x] Export to Parquet: `papers`, `authorships`, `citations`, `coauthorships`
- [x] Create comprehensive data dictionary (377 lines)
- [x] Data profiling and quality analysis notebook
- [x] Schema normalization with author name standardization

### Phase 2: Network Analysis (Nhien) 
- [x] Build Citation Graph (Directed) & Co-authorship Graph (Undirected)
- [x] Compute Centralities: Degree, PageRank, Betweenness
- [x] Detect Communities (Louvain) and track temporal evolution
- [x] Generate network visualizations and metrics tables

### Phase 3: NLP & Modeling (Julio) 
- [ ] Text Features: TF-IDF (10K features) -> PCA
- [ ] Topic Modeling: LDA/NMF to identify subfields
- [ ] Predictive Task: Forecast citation impact (Pre-2010 train / Post-2010 test)
- [ ] Anomaly detection in publications and collaborations

### Phase 4: Integration & Reporting (All) 
- [ ] Combine insights from all analyses
- [ ] Create publication-ready figures and tables
- [ ] Write final report with findings and recommendations
- [ ] Prepare presentation materials
